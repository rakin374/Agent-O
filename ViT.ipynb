{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Transformer for Skin Cancer Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import safetensors.torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing Data\n",
    "This code grabs the image data for the competition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"isic-2024-challenge\"\n",
    "\n",
    "# Train + Valid\n",
    "df = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\n",
    "df = df.ffill()\n",
    "display(df.head(2))\n",
    "\n",
    "# Testing\n",
    "testing_df = pd.read_csv(f'{BASE_PATH}/test-metadata.csv')\n",
    "testing_df = testing_df.ffill()\n",
    "display(testing_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "You'll notice the dataset is heavily skewed towards 'target:0'. If you run the following code block, you'll see the Class Distribution Before Sampling (%):\n",
    "\n",
    "    target_0:99.902009\n",
    "\n",
    "    target_1:0.097991\n",
    "\n",
    "The following code redistributes the data such that the ratio of 'target:0' and 'target:1' is much closer:\n",
    "\n",
    "    target_0:67.09645\n",
    "\n",
    "    target_1:32.90355\n",
    "\n",
    "We will use this data in our following steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handle Class Imbalance\n",
    "print(\"Class Distribution Before Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "seed = 1\n",
    "neg_sample = .01\n",
    "pos_sample = 5.0\n",
    "# Sampling\n",
    "positive_df = df.query(\"target==0\").sample(frac=neg_sample, random_state=seed)\n",
    "negative_df = df.query(\"target==1\").sample(frac=pos_sample, replace=True, random_state=seed)\n",
    "df = pd.concat([positive_df, negative_df], axis=0).sample(frac=1.0)\n",
    "\n",
    "print(\"\\nClass Distribution After Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "\n",
    "# Assume df is your DataFrame and 'target' is the column with class labels\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df['target']), y=df['target'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation, and Test Data\n",
    "\n",
    "This script splits our data into training (60%), validation (20%), and test (20%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "training_validation_hdf5 = h5py.File(f\"{BASE_PATH}/train-image.hdf5\", 'r')\n",
    "\n",
    "# Reset index to ensure a continuous index\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"fold\"] = -1\n",
    "\n",
    "# Set up the StratifiedGroupKFold with 5 splits\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Assign fold numbers to each data point\n",
    "for i, (training_idx, validation_idx) in enumerate(sgkf.split(df, y=df.target, groups=df.patient_id)):\n",
    "    df.loc[validation_idx, \"fold\"] = int(i)\n",
    "\n",
    "# Define the train, validation, and test sets\n",
    "# Use fold 0 for test, fold 1 for validation, and remaining folds for training\n",
    "training_df = df.query(\"fold > 1\")  # Folds 2, 3, 4 for training\n",
    "validation_df = df.query(\"fold == 1\")  # Fold 1 for validation\n",
    "test_df = df.query(\"fold == 0\")  # Fold 0 for testing\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(f\"# Num Train: {len(training_df)} | Num Valid: {len(validation_df)} | Num Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "Here we instantiate the Visual Transformer (ViT). The model takes in 224x224 images, so some transformations are needed for the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "\n",
    "def preprocess_images(example):\n",
    "        byte_string = training_validation_hdf5[example[\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        example['pixel_values'] = transform(image)\n",
    "        return example\n",
    "\n",
    "ds_training = datasets.Dataset.from_pandas(pd.DataFrame(data=training_df))\n",
    "ds_valid = datasets.Dataset.from_pandas(pd.DataFrame(data=validation_df))\n",
    "ds_test = datasets.Dataset.from_pandas(pd.DataFrame(data=test_df))\n",
    "\n",
    "\n",
    "ds_training = ds_training.map(preprocess_images, batched=False)\n",
    "ds_valid = ds_valid.map(preprocess_images, batched=False)\n",
    "ds_test = ds_test.map(preprocess_images, batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "For a classification problem, we use an accuracy metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Make sure its the right tensor types\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(example['pixel_values']) for example in batch])\n",
    "    labels = torch.tensor([example['target'] for example in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vit-finetuned-agent0',\n",
    "    metric_for_best_model = \"accuracy\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=20,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_training,\n",
    "    eval_dataset=ds_valid,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train da model\n",
    "trainer.train()\n",
    "# Save da fine-tuned model\n",
    "trainer.save_model('./vit-finetuned-agentO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.eval_dataset=ds_test\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Model\n",
    "This code evaulates the final model on the test dataset and plots the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_set(ds, num_samples):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(num_samples):\n",
    "        byte_string = training_validation_hdf5[ds[i][\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        inputs.append(image)\n",
    "        labels.append(ds[i][\"target\"]) \n",
    "    return inputs, labels\n",
    "\n",
    "inputs, y_true = process_test_set(ds_test, 1000)\n",
    "print(inputs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "state_dict = safetensors.torch.load_file(\"vit-finetuned-agent0/checkpoint-228/model.safetensors\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "image_classifier = pipeline(\"image-classification\", model,image_processor=processor)\n",
    "predictions = image_classifier(inputs)\n",
    "#select highest labels\n",
    "predictions = [max(item, key=lambda x: x['score'])['label'] for item in predictions]\n",
    "#convert from LABEL_0, LABEL_1 to 0,1\n",
    "predictions = [1 if item == 'LABEL_1' else 0 for item in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot confusion matrix\n",
    "confusion_metric = evaluate.load(\"confusion_matrix\")\n",
    "confusion_matrix = confusion_metric.compute(predictions=predictions, references=y_true)\n",
    "matrix = confusion_matrix['confusion_matrix']\n",
    "if 'labels' in confusion_matrix:\n",
    "    labels = confusion_matrix['labels']\n",
    "else:\n",
    "    labels = np.unique(predictions + y_true)\n",
    "sns.heatmap(matrix, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
