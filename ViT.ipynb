{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Transformer for Skin Cancer Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 11:34:07.477388: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730388847.496956  282034 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730388847.503028  282034 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 11:34:07.525504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import safetensors.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/291154.1.ood/ipykernel_282034/4180780485.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>image_type</th>\n",
       "      <th>tbp_tile_type</th>\n",
       "      <th>tbp_lv_A</th>\n",
       "      <th>...</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>iddx_full</th>\n",
       "      <th>iddx_1</th>\n",
       "      <th>iddx_2</th>\n",
       "      <th>iddx_3</th>\n",
       "      <th>iddx_4</th>\n",
       "      <th>iddx_5</th>\n",
       "      <th>mel_mitotic_index</th>\n",
       "      <th>mel_thick_mm</th>\n",
       "      <th>tbp_lv_dnn_lesion_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015670</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_1235828</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>3.04</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: white</td>\n",
       "      <td>20.244422</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Benign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.517282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015845</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_8170065</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>1.10</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: white</td>\n",
       "      <td>31.712570</td>\n",
       "      <td>...</td>\n",
       "      <td>IL_6727506</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Benign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.141455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  target  patient_id  age_approx   sex anatom_site_general  \\\n",
       "0  ISIC_0015670       0  IP_1235828        60.0  male     lower extremity   \n",
       "1  ISIC_0015845       0  IP_8170065        60.0  male           head/neck   \n",
       "\n",
       "   clin_size_long_diam_mm          image_type tbp_tile_type   tbp_lv_A  ...  \\\n",
       "0                    3.04  TBP tile: close-up     3D: white  20.244422  ...   \n",
       "1                    1.10  TBP tile: close-up     3D: white  31.712570  ...   \n",
       "\n",
       "    lesion_id  iddx_full  iddx_1  iddx_2  iddx_3  iddx_4  iddx_5  \\\n",
       "0         NaN     Benign  Benign     NaN     NaN     NaN     NaN   \n",
       "1  IL_6727506     Benign  Benign     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   mel_mitotic_index  mel_thick_mm  tbp_lv_dnn_lesion_confidence  \n",
       "0                NaN           NaN                     97.517282  \n",
       "1                NaN           NaN                      3.141455  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>image_type</th>\n",
       "      <th>tbp_tile_type</th>\n",
       "      <th>tbp_lv_A</th>\n",
       "      <th>tbp_lv_Aext</th>\n",
       "      <th>...</th>\n",
       "      <th>tbp_lv_radial_color_std_max</th>\n",
       "      <th>tbp_lv_stdL</th>\n",
       "      <th>tbp_lv_stdLExt</th>\n",
       "      <th>tbp_lv_symm_2axis</th>\n",
       "      <th>tbp_lv_symm_2axis_angle</th>\n",
       "      <th>tbp_lv_x</th>\n",
       "      <th>tbp_lv_y</th>\n",
       "      <th>tbp_lv_z</th>\n",
       "      <th>attribution</th>\n",
       "      <th>copyright_license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>IP_6074337</td>\n",
       "      <td>45.0</td>\n",
       "      <td>male</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>2.70</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: XP</td>\n",
       "      <td>22.80433</td>\n",
       "      <td>20.007270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304827</td>\n",
       "      <td>1.281532</td>\n",
       "      <td>2.299935</td>\n",
       "      <td>0.479339</td>\n",
       "      <td>20</td>\n",
       "      <td>-155.06510</td>\n",
       "      <td>1511.222000</td>\n",
       "      <td>113.980100</td>\n",
       "      <td>Memorial Sloan Kettering Cancer Center</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>IP_1664139</td>\n",
       "      <td>35.0</td>\n",
       "      <td>female</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>2.52</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: XP</td>\n",
       "      <td>16.64867</td>\n",
       "      <td>9.657964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.271940</td>\n",
       "      <td>2.011223</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>25</td>\n",
       "      <td>-112.36924</td>\n",
       "      <td>629.535889</td>\n",
       "      <td>-15.019287</td>\n",
       "      <td>Frazer Institute, The University of Queensland...</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  patient_id  age_approx     sex anatom_site_general  \\\n",
       "0  ISIC_0015657  IP_6074337        45.0    male     posterior torso   \n",
       "1  ISIC_0015729  IP_1664139        35.0  female     lower extremity   \n",
       "\n",
       "   clin_size_long_diam_mm          image_type tbp_tile_type  tbp_lv_A  \\\n",
       "0                    2.70  TBP tile: close-up        3D: XP  22.80433   \n",
       "1                    2.52  TBP tile: close-up        3D: XP  16.64867   \n",
       "\n",
       "   tbp_lv_Aext  ...  tbp_lv_radial_color_std_max  tbp_lv_stdL  tbp_lv_stdLExt  \\\n",
       "0    20.007270  ...                     0.304827     1.281532        2.299935   \n",
       "1     9.657964  ...                     0.000000     1.271940        2.011223   \n",
       "\n",
       "   tbp_lv_symm_2axis  tbp_lv_symm_2axis_angle   tbp_lv_x     tbp_lv_y  \\\n",
       "0           0.479339                       20 -155.06510  1511.222000   \n",
       "1           0.426230                       25 -112.36924   629.535889   \n",
       "\n",
       "     tbp_lv_z                                        attribution  \\\n",
       "0  113.980100             Memorial Sloan Kettering Cancer Center   \n",
       "1  -15.019287  Frazer Institute, The University of Queensland...   \n",
       "\n",
       "   copyright_license  \n",
       "0              CC-BY  \n",
       "1              CC-BY  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading data\n",
    "BASE_PATH = \"isic-2024-challenge\"\n",
    "\n",
    "# Train + Valid\n",
    "df = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\n",
    "df = df.ffill()\n",
    "display(df.head(2))\n",
    "\n",
    "# Testing\n",
    "testing_df = pd.read_csv(f'{BASE_PATH}/test-metadata.csv')\n",
    "testing_df = testing_df.ffill()\n",
    "display(testing_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    99.902009\n",
       "1     0.097991\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution After Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    67.09645\n",
       "1    32.90355\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.7451959071624656, 1: 1.519592875318066}\n"
     ]
    }
   ],
   "source": [
    "#Handle Class Imbalance\n",
    "print(\"Class Distribution Before Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "seed = 1\n",
    "neg_sample = .01\n",
    "pos_sample = 5.0\n",
    "# Sampling\n",
    "positive_df = df.query(\"target==0\").sample(frac=neg_sample, random_state=seed)\n",
    "negative_df = df.query(\"target==1\").sample(frac=pos_sample, replace=True, random_state=seed)\n",
    "df = pd.concat([positive_df, negative_df], axis=0).sample(frac=1.0)\n",
    "\n",
    "print(\"\\nClass Distribution After Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "\n",
    "# Assume df is your DataFrame and 'target' is the column with class labels\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df['target']), y=df['target'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_validation_hdf5 = h5py.File(f\"{BASE_PATH}/train-image.hdf5\", 'r')\n",
    "testing_hdf5 = h5py.File(f\"{BASE_PATH}/test-image.hdf5\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num Train: 4713 | Num Valid: 1259\n"
     ]
    }
   ],
   "source": [
    "#split into train and validation sets\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "df = df.reset_index(drop=True) # ensure continuous index\n",
    "df[\"fold\"] = -1\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for i, (training_idx, validation_idx) in enumerate(sgkf.split(df, y=df.target, groups=df.patient_id)):\n",
    "    df.loc[validation_idx, \"fold\"] = int(i)\n",
    "\n",
    "# Use first fold for training and validation\n",
    "training_df = df.query(\"fold!=0\")\n",
    "validation_df = df.query(\"fold==0\")\n",
    "print(f\"# Num Train: {len(training_df)} | Num Valid: {len(validation_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_images at 0x14e4daecf420> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff19fed1dd9843e5901578a36ee79173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4713 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7eb4d6d6c294f79a8e389ab41fc56aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "\n",
    "def preprocess_images(example):\n",
    "        byte_string = training_validation_hdf5[example[\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        example['pixel_values'] = transform(image)\n",
    "        return example\n",
    "\n",
    "\n",
    "\n",
    "ds_training = datasets.Dataset.from_pandas(pd.DataFrame(data=training_df))\n",
    "ds_valid = datasets.Dataset.from_pandas(pd.DataFrame(data=validation_df))\n",
    "\n",
    "ds_training = ds_training.map(preprocess_images, batched=False)\n",
    "ds_valid = ds_valid.map(preprocess_images, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/projectnb/cs640grp/students/samwu/AgentO/.venv/lib64/python3.11/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/scratch/291154.1.ood/ipykernel_282034/1110159578.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "#load/preprosses dataset\n",
    "model_name='google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "#load the model from google\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Make sure its the right tensor types\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(example['pixel_values']) for example in batch])\n",
    "    labels = torch.tensor([example['target'] for example in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vit-finetuned-agent0',\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_training,\n",
    "    eval_dataset=ds_valid,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='296' max='1180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 296/1180 05:50 < 17:34, 0.84 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/158 01:24 < 00:01, 1.84 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train da model\n",
    "trainer.train()\n",
    "\n",
    "# Save da fine-tuned model\n",
    "trainer.save_model('./vit-finetuned-agentO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/158 01:25 < 00:01, 1.81 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0206170082092285,\n",
       " 'eval_accuracy': 0.7903097696584591,\n",
       " 'eval_runtime': 87.456,\n",
       " 'eval_samples_per_second': 14.396,\n",
       " 'eval_steps_per_second': 1.807,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset=ds_valid\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.Image.Image image mode=RGB size=139x139 at 0x14E4DBB91990>, <PIL.Image.Image image mode=RGB size=129x129 at 0x14E4DF5E8D50>, <PIL.Image.Image image mode=RGB size=127x127 at 0x14E4DAFBD190>, <PIL.Image.Image image mode=RGB size=115x115 at 0x14E4DAF15E50>, <PIL.Image.Image image mode=RGB size=119x119 at 0x14E4DAE17CD0>, <PIL.Image.Image image mode=RGB size=137x137 at 0x14E4DAF01C10>, <PIL.Image.Image image mode=RGB size=137x137 at 0x14E4DAE29610>, <PIL.Image.Image image mode=RGB size=149x149 at 0x14E4DAE2B550>, <PIL.Image.Image image mode=RGB size=111x111 at 0x14E4DAD806D0>, <PIL.Image.Image image mode=RGB size=129x129 at 0x14E4DACD9AD0>]\n"
     ]
    }
   ],
   "source": [
    "ds_valid = datasets.Dataset.from_pandas(pd.DataFrame(data=validation_df))\n",
    "\n",
    "def process_test_set(ds, num_samples):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(num_samples):\n",
    "        byte_string = training_validation_hdf5[ds[i][\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        inputs.append(image)\n",
    "        labels.append(ds[i][\"target\"]) \n",
    "    return inputs, labels\n",
    "\n",
    "inputs, y_true = process_test_set(ds_valid, 100)\n",
    "print(inputs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "state_dict = safetensors.torch.load_file(\"vit-finetuned-agentO/model.safetensors\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "image_classifier = pipeline(\"image-classification\", model,image_processor=processor)\n",
    "predictions = image_classifier(inputs)\n",
    "#select highest labels\n",
    "predictions = [max(item, key=lambda x: x['score'])['label'] for item in predictions]\n",
    "#convert from LABEL_0, LABEL_1 to 0,1\n",
    "predictions = [1 if item == 'LABEL_1' else 0 for item in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "confusion_metric = evaluate.load(\"confusion_matrix\")\n",
    "confusion_matrix = confusion_metric.compute(predictions=predictions, references=y_true)\n",
    "matrix = confusion_matrix['confusion_matrix']\n",
    "if 'labels' in confusion_matrix:\n",
    "    labels = confusion_matrix['labels']\n",
    "else:\n",
    "    labels = np.unique(predictions + y_true)\n",
    "sns.heatmap(matrix, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
