{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Transformer for Skin Cancer Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import safetensors.torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/300484.1.ood/ipykernel_1502367/4180780485.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>image_type</th>\n",
       "      <th>tbp_tile_type</th>\n",
       "      <th>tbp_lv_A</th>\n",
       "      <th>...</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>iddx_full</th>\n",
       "      <th>iddx_1</th>\n",
       "      <th>iddx_2</th>\n",
       "      <th>iddx_3</th>\n",
       "      <th>iddx_4</th>\n",
       "      <th>iddx_5</th>\n",
       "      <th>mel_mitotic_index</th>\n",
       "      <th>mel_thick_mm</th>\n",
       "      <th>tbp_lv_dnn_lesion_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015670</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_1235828</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>3.04</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: white</td>\n",
       "      <td>20.244422</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Benign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.517282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015845</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_8170065</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>1.10</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: white</td>\n",
       "      <td>31.712570</td>\n",
       "      <td>...</td>\n",
       "      <td>IL_6727506</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Benign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.141455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  target  patient_id  age_approx   sex anatom_site_general  \\\n",
       "0  ISIC_0015670       0  IP_1235828        60.0  male     lower extremity   \n",
       "1  ISIC_0015845       0  IP_8170065        60.0  male           head/neck   \n",
       "\n",
       "   clin_size_long_diam_mm          image_type tbp_tile_type   tbp_lv_A  ...  \\\n",
       "0                    3.04  TBP tile: close-up     3D: white  20.244422  ...   \n",
       "1                    1.10  TBP tile: close-up     3D: white  31.712570  ...   \n",
       "\n",
       "    lesion_id  iddx_full  iddx_1  iddx_2  iddx_3  iddx_4  iddx_5  \\\n",
       "0         NaN     Benign  Benign     NaN     NaN     NaN     NaN   \n",
       "1  IL_6727506     Benign  Benign     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   mel_mitotic_index  mel_thick_mm  tbp_lv_dnn_lesion_confidence  \n",
       "0                NaN           NaN                     97.517282  \n",
       "1                NaN           NaN                      3.141455  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>image_type</th>\n",
       "      <th>tbp_tile_type</th>\n",
       "      <th>tbp_lv_A</th>\n",
       "      <th>tbp_lv_Aext</th>\n",
       "      <th>...</th>\n",
       "      <th>tbp_lv_radial_color_std_max</th>\n",
       "      <th>tbp_lv_stdL</th>\n",
       "      <th>tbp_lv_stdLExt</th>\n",
       "      <th>tbp_lv_symm_2axis</th>\n",
       "      <th>tbp_lv_symm_2axis_angle</th>\n",
       "      <th>tbp_lv_x</th>\n",
       "      <th>tbp_lv_y</th>\n",
       "      <th>tbp_lv_z</th>\n",
       "      <th>attribution</th>\n",
       "      <th>copyright_license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>IP_6074337</td>\n",
       "      <td>45.0</td>\n",
       "      <td>male</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>2.70</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: XP</td>\n",
       "      <td>22.80433</td>\n",
       "      <td>20.007270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304827</td>\n",
       "      <td>1.281532</td>\n",
       "      <td>2.299935</td>\n",
       "      <td>0.479339</td>\n",
       "      <td>20</td>\n",
       "      <td>-155.06510</td>\n",
       "      <td>1511.222000</td>\n",
       "      <td>113.980100</td>\n",
       "      <td>Memorial Sloan Kettering Cancer Center</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>IP_1664139</td>\n",
       "      <td>35.0</td>\n",
       "      <td>female</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>2.52</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: XP</td>\n",
       "      <td>16.64867</td>\n",
       "      <td>9.657964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.271940</td>\n",
       "      <td>2.011223</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>25</td>\n",
       "      <td>-112.36924</td>\n",
       "      <td>629.535889</td>\n",
       "      <td>-15.019287</td>\n",
       "      <td>Frazer Institute, The University of Queensland...</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  patient_id  age_approx     sex anatom_site_general  \\\n",
       "0  ISIC_0015657  IP_6074337        45.0    male     posterior torso   \n",
       "1  ISIC_0015729  IP_1664139        35.0  female     lower extremity   \n",
       "\n",
       "   clin_size_long_diam_mm          image_type tbp_tile_type  tbp_lv_A  \\\n",
       "0                    2.70  TBP tile: close-up        3D: XP  22.80433   \n",
       "1                    2.52  TBP tile: close-up        3D: XP  16.64867   \n",
       "\n",
       "   tbp_lv_Aext  ...  tbp_lv_radial_color_std_max  tbp_lv_stdL  tbp_lv_stdLExt  \\\n",
       "0    20.007270  ...                     0.304827     1.281532        2.299935   \n",
       "1     9.657964  ...                     0.000000     1.271940        2.011223   \n",
       "\n",
       "   tbp_lv_symm_2axis  tbp_lv_symm_2axis_angle   tbp_lv_x     tbp_lv_y  \\\n",
       "0           0.479339                       20 -155.06510  1511.222000   \n",
       "1           0.426230                       25 -112.36924   629.535889   \n",
       "\n",
       "     tbp_lv_z                                        attribution  \\\n",
       "0  113.980100             Memorial Sloan Kettering Cancer Center   \n",
       "1  -15.019287  Frazer Institute, The University of Queensland...   \n",
       "\n",
       "   copyright_license  \n",
       "0              CC-BY  \n",
       "1              CC-BY  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading data\n",
    "BASE_PATH = \"isic-2024-challenge\"\n",
    "\n",
    "# Train + Valid\n",
    "df = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\n",
    "df = df.ffill()\n",
    "display(df.head(2))\n",
    "\n",
    "# Testing\n",
    "testing_df = pd.read_csv(f'{BASE_PATH}/test-metadata.csv')\n",
    "testing_df = testing_df.ffill()\n",
    "display(testing_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    99.902009\n",
       "1     0.097991\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution After Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    67.09645\n",
       "1    32.90355\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.7451959071624656, 1: 1.519592875318066}\n"
     ]
    }
   ],
   "source": [
    "#Handle Class Imbalance\n",
    "print(\"Class Distribution Before Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "seed = 1\n",
    "neg_sample = .01\n",
    "pos_sample = 5.0\n",
    "# Sampling\n",
    "positive_df = df.query(\"target==0\").sample(frac=neg_sample, random_state=seed)\n",
    "negative_df = df.query(\"target==1\").sample(frac=pos_sample, replace=True, random_state=seed)\n",
    "df = pd.concat([positive_df, negative_df], axis=0).sample(frac=1.0)\n",
    "\n",
    "print(\"\\nClass Distribution After Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "\n",
    "# Assume df is your DataFrame and 'target' is the column with class labels\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df['target']), y=df['target'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "def get_k_data(df, k):\n",
    "    training_validation_hdf5 = h5py.File(f\"{BASE_PATH}/train-image.hdf5\", 'r')\n",
    "\n",
    "    # Reset index to ensure a continuous index\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"fold\"] = -1\n",
    "\n",
    "    # Set up the StratifiedGroupKFold with 5 splits\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Assign fold numbers to each data point\n",
    "    for i, (training_idx, validation_idx) in enumerate(sgkf.split(df, y=df.target, groups=df.patient_id)):\n",
    "        df.loc[validation_idx, \"fold\"] = int(i)\n",
    "    # Define the train, validation, and test sets\n",
    "    # Use fold 0 for test, fold 1 for validation, and remaining folds for training\n",
    "    #change these for each trial, testing all test_fold = [1,2,3,4,5]\n",
    "    test_fold = k\n",
    "    valid_fold = random.choice([num for num in range(5) if num != k])\n",
    "\n",
    "    # Create the training, validation, and test sets\n",
    "    training_df = df.query(f\"fold != {valid_fold} and fold != {test_fold}\")\n",
    "    validation_df = df.query(f\"fold == {valid_fold}\")\n",
    "    test_df = df.query(f\"fold == {test_fold}\")\n",
    "\n",
    "    # Print the number of samples in each set\n",
    "    print(f\"# Num Train: {len(training_df)} | Num Valid: {len(validation_df)} | Num Test: {len(test_df)}\")\n",
    "    return training_df, validation_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "\n",
    "def preprocess_images(example):\n",
    "        byte_string = training_validation_hdf5[example[\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        example['pixel_values'] = transform(image)\n",
    "        return example\n",
    "\n",
    "def get_datasets(df, k):\n",
    "        training_df,validation_df,test_df = get_k_data(df, k)\n",
    "\n",
    "        ds_training = datasets.Dataset.from_pandas(pd.DataFrame(data=training_df))\n",
    "        ds_valid = datasets.Dataset.from_pandas(pd.DataFrame(data=validation_df))\n",
    "        ds_test = datasets.Dataset.from_pandas(pd.DataFrame(data=test_df))\n",
    "\n",
    "        ds_training = ds_training.map(preprocess_images, batched=False)\n",
    "        ds_valid = ds_valid.map(preprocess_images, batched=False)\n",
    "        ds_test = ds_test.map(preprocess_images, batched=False)\n",
    "\n",
    "        return ds_training, ds_valid, ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6222dc42c2424406bdd841d9feb9cc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_set(ds, num_samples):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(num_samples):\n",
    "        byte_string = training_validation_hdf5[ds[i][\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        inputs.append(image)\n",
    "        labels.append(ds[i][\"target\"]) \n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validate(model):\n",
    "    inputs, y_true = process_test_set(ds_test, 1000)\n",
    "\n",
    "    image_classifier = pipeline(\"image-classification\", model ,image_processor=processor)\n",
    "    predictions = image_classifier(inputs)\n",
    "    #select highest labels\n",
    "    predictions = [max(item, key=lambda x: x['score'])['label'] for item in predictions]\n",
    "    #convert from LABEL_0, LABEL_1 to 0,1\n",
    "    predictions = [1 if item == 'LABEL_1' else 0 for item in predictions]  \n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    #plot confusion matrix\n",
    "    confusion_metric = evaluate.load(\"confusion_matrix\")\n",
    "    confusion_matrix = confusion_metric.compute(predictions=predictions, references=y_true)\n",
    "    matrix = confusion_matrix['confusion_matrix']\n",
    "    if 'labels' in confusion_matrix:\n",
    "        labels = confusion_matrix['labels']\n",
    "    else:\n",
    "        labels = np.unique(predictions + y_true)\n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()  \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num Train: 3426 | Num Valid: 1287 | Num Test: 1259\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e791a6d72bd4cc980f272b6f46a0856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209cf85962e0415a9deaad1da51be82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d9468904884fbdaa408f87ccd15b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/cs640grp/students/samwu/AgentO/.venv/lib64/python3.11/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/scratch/300484.1.ood/ipykernel_1502367/742869072.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='4300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   6/4300 01:26 < 25:52:14, 0.05 it/s, Epoch 0.02/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Make sure its the right tensor types\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(example['pixel_values']) for example in batch])\n",
    "    labels = torch.tensor([example['target'] for example in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "\n",
    "for k in range(1):\n",
    "    ds_training, ds_valid, ds_test = get_datasets(df, k)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./vit-finetuned-agent0',\n",
    "        metric_for_best_model = \"accuracy\",\n",
    "        per_device_train_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=20,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_training,\n",
    "        eval_dataset=ds_valid,\n",
    "        data_collator=collate_fn,\n",
    "        tokenizer=processor,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    #train da model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save da fine-tuned model\n",
    "    trainer.save_model('./vit-finetuned-agentO')\n",
    "\n",
    "    trainer.eval_dataset=ds_test\n",
    "    trainer.evaluate()\n",
    "\n",
    "    test_validate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
