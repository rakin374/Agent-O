{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Transformer for Skin Cancer Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 12:21:48.359080: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-01 12:21:48.368631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730478108.379622 4185756 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730478108.382958 4185756 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-01 12:21:48.395534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "from transformers import ViTForImageClassification, Trainer, TrainingArguments\n",
    "from transformers import ViTImageProcessor\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import h5py\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import safetensors.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/914532.1.l40s/ipykernel_4185756/4180780485.py:5: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>image_type</th>\n",
       "      <th>tbp_tile_type</th>\n",
       "      <th>tbp_lv_A</th>\n",
       "      <th>...</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>iddx_full</th>\n",
       "      <th>iddx_1</th>\n",
       "      <th>iddx_2</th>\n",
       "      <th>iddx_3</th>\n",
       "      <th>iddx_4</th>\n",
       "      <th>iddx_5</th>\n",
       "      <th>mel_mitotic_index</th>\n",
       "      <th>mel_thick_mm</th>\n",
       "      <th>tbp_lv_dnn_lesion_confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015670</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_1235828</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>3.04</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: white</td>\n",
       "      <td>20.244422</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Benign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.517282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015845</td>\n",
       "      <td>0</td>\n",
       "      <td>IP_8170065</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>1.10</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: white</td>\n",
       "      <td>31.712570</td>\n",
       "      <td>...</td>\n",
       "      <td>IL_6727506</td>\n",
       "      <td>Benign</td>\n",
       "      <td>Benign</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.141455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  target  patient_id  age_approx   sex anatom_site_general  \\\n",
       "0  ISIC_0015670       0  IP_1235828        60.0  male     lower extremity   \n",
       "1  ISIC_0015845       0  IP_8170065        60.0  male           head/neck   \n",
       "\n",
       "   clin_size_long_diam_mm          image_type tbp_tile_type   tbp_lv_A  ...  \\\n",
       "0                    3.04  TBP tile: close-up     3D: white  20.244422  ...   \n",
       "1                    1.10  TBP tile: close-up     3D: white  31.712570  ...   \n",
       "\n",
       "    lesion_id  iddx_full  iddx_1  iddx_2  iddx_3  iddx_4  iddx_5  \\\n",
       "0         NaN     Benign  Benign     NaN     NaN     NaN     NaN   \n",
       "1  IL_6727506     Benign  Benign     NaN     NaN     NaN     NaN   \n",
       "\n",
       "   mel_mitotic_index  mel_thick_mm  tbp_lv_dnn_lesion_confidence  \n",
       "0                NaN           NaN                     97.517282  \n",
       "1                NaN           NaN                      3.141455  \n",
       "\n",
       "[2 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>clin_size_long_diam_mm</th>\n",
       "      <th>image_type</th>\n",
       "      <th>tbp_tile_type</th>\n",
       "      <th>tbp_lv_A</th>\n",
       "      <th>tbp_lv_Aext</th>\n",
       "      <th>...</th>\n",
       "      <th>tbp_lv_radial_color_std_max</th>\n",
       "      <th>tbp_lv_stdL</th>\n",
       "      <th>tbp_lv_stdLExt</th>\n",
       "      <th>tbp_lv_symm_2axis</th>\n",
       "      <th>tbp_lv_symm_2axis_angle</th>\n",
       "      <th>tbp_lv_x</th>\n",
       "      <th>tbp_lv_y</th>\n",
       "      <th>tbp_lv_z</th>\n",
       "      <th>attribution</th>\n",
       "      <th>copyright_license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>IP_6074337</td>\n",
       "      <td>45.0</td>\n",
       "      <td>male</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>2.70</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: XP</td>\n",
       "      <td>22.80433</td>\n",
       "      <td>20.007270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304827</td>\n",
       "      <td>1.281532</td>\n",
       "      <td>2.299935</td>\n",
       "      <td>0.479339</td>\n",
       "      <td>20</td>\n",
       "      <td>-155.06510</td>\n",
       "      <td>1511.222000</td>\n",
       "      <td>113.980100</td>\n",
       "      <td>Memorial Sloan Kettering Cancer Center</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>IP_1664139</td>\n",
       "      <td>35.0</td>\n",
       "      <td>female</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>2.52</td>\n",
       "      <td>TBP tile: close-up</td>\n",
       "      <td>3D: XP</td>\n",
       "      <td>16.64867</td>\n",
       "      <td>9.657964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.271940</td>\n",
       "      <td>2.011223</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>25</td>\n",
       "      <td>-112.36924</td>\n",
       "      <td>629.535889</td>\n",
       "      <td>-15.019287</td>\n",
       "      <td>Frazer Institute, The University of Queensland...</td>\n",
       "      <td>CC-BY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id  patient_id  age_approx     sex anatom_site_general  \\\n",
       "0  ISIC_0015657  IP_6074337        45.0    male     posterior torso   \n",
       "1  ISIC_0015729  IP_1664139        35.0  female     lower extremity   \n",
       "\n",
       "   clin_size_long_diam_mm          image_type tbp_tile_type  tbp_lv_A  \\\n",
       "0                    2.70  TBP tile: close-up        3D: XP  22.80433   \n",
       "1                    2.52  TBP tile: close-up        3D: XP  16.64867   \n",
       "\n",
       "   tbp_lv_Aext  ...  tbp_lv_radial_color_std_max  tbp_lv_stdL  tbp_lv_stdLExt  \\\n",
       "0    20.007270  ...                     0.304827     1.281532        2.299935   \n",
       "1     9.657964  ...                     0.000000     1.271940        2.011223   \n",
       "\n",
       "   tbp_lv_symm_2axis  tbp_lv_symm_2axis_angle   tbp_lv_x     tbp_lv_y  \\\n",
       "0           0.479339                       20 -155.06510  1511.222000   \n",
       "1           0.426230                       25 -112.36924   629.535889   \n",
       "\n",
       "     tbp_lv_z                                        attribution  \\\n",
       "0  113.980100             Memorial Sloan Kettering Cancer Center   \n",
       "1  -15.019287  Frazer Institute, The University of Queensland...   \n",
       "\n",
       "   copyright_license  \n",
       "0              CC-BY  \n",
       "1              CC-BY  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading data\n",
    "BASE_PATH = \"isic-2024-challenge\"\n",
    "\n",
    "# Train + Valid\n",
    "df = pd.read_csv(f'{BASE_PATH}/train-metadata.csv')\n",
    "df = df.ffill()\n",
    "display(df.head(2))\n",
    "\n",
    "# Testing\n",
    "testing_df = pd.read_csv(f'{BASE_PATH}/test-metadata.csv')\n",
    "testing_df = testing_df.ffill()\n",
    "display(testing_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    99.902009\n",
       "1     0.097991\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class Distribution After Sampling (%):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    67.09645\n",
       "1    32.90355\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.7451959071624656, 1: 1.519592875318066}\n"
     ]
    }
   ],
   "source": [
    "#Handle Class Imbalance\n",
    "print(\"Class Distribution Before Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "seed = 1\n",
    "neg_sample = .01\n",
    "pos_sample = 5.0\n",
    "# Sampling\n",
    "positive_df = df.query(\"target==0\").sample(frac=neg_sample, random_state=seed)\n",
    "negative_df = df.query(\"target==1\").sample(frac=pos_sample, replace=True, random_state=seed)\n",
    "df = pd.concat([positive_df, negative_df], axis=0).sample(frac=1.0)\n",
    "\n",
    "print(\"\\nClass Distribution After Sampling (%):\")\n",
    "display(df.target.value_counts(normalize=True)*100)\n",
    "\n",
    "# Assume df is your DataFrame and 'target' is the column with class labels\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(df['target']), y=df['target'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num Train: 3647 | Num Valid: 1066 | Num Test: 1259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "training_validation_hdf5 = h5py.File(f\"{BASE_PATH}/train-image.hdf5\", 'r')\n",
    "\n",
    "# Reset index to ensure a continuous index\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"fold\"] = -1\n",
    "\n",
    "# Set up the StratifiedGroupKFold with 5 splits\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Assign fold numbers to each data point\n",
    "for i, (training_idx, validation_idx) in enumerate(sgkf.split(df, y=df.target, groups=df.patient_id)):\n",
    "    df.loc[validation_idx, \"fold\"] = int(i)\n",
    "\n",
    "# Define the train, validation, and test sets\n",
    "# Use fold 0 for test, fold 1 for validation, and remaining folds for training\n",
    "training_df = df.query(\"fold > 1\")  # Folds 2, 3, 4 for training\n",
    "validation_df = df.query(\"fold == 1\")  # Fold 1 for validation\n",
    "test_df = df.query(\"fold == 0\")  # Fold 0 for testing\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(f\"# Num Train: {len(training_df)} | Num Valid: {len(validation_df)} | Num Test: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_images at 0x15513937da80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6951099d14e7457fb7ccdd3895b10993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a0644762bf4dd19d0d4d5e5e397ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5e4136d0ba4567a31b762369494dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "transform = Compose([\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "\n",
    "def preprocess_images(example):\n",
    "        byte_string = training_validation_hdf5[example[\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        example['pixel_values'] = transform(image)\n",
    "        return example\n",
    "\n",
    "ds_training = datasets.Dataset.from_pandas(pd.DataFrame(data=training_df))\n",
    "ds_valid = datasets.Dataset.from_pandas(pd.DataFrame(data=validation_df))\n",
    "ds_test = datasets.Dataset.from_pandas(pd.DataFrame(data=test_df))\n",
    "\n",
    "\n",
    "ds_training = ds_training.map(preprocess_images, batched=False)\n",
    "ds_valid = ds_valid.map(preprocess_images, batched=False)\n",
    "ds_test = ds_test.map(preprocess_images, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/projectnb/cs640grp/students/samwu/AgentO/.venv/lib64/python3.11/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/scratch/914532.1.l40s/ipykernel_4185756/2587322362.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "model_name='google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Make sure its the right tensor types\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.tensor(example['pixel_values']) for example in batch])\n",
    "    labels = torch.tensor([example['target'] for example in batch])\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./vit-finetuned-agent0',\n",
    "    metric_for_best_model = \"accuracy\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=20,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_training,\n",
    "    eval_dataset=ds_valid,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='229' max='4560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 229/4560 02:44 < 52:16, 1.38 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/134 00:43 < 00:00, 2.99 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train da model\n",
    "trainer.train()\n",
    "\n",
    "# Save da fine-tuned model\n",
    "trainer.save_model('./vit-finetuned-agentO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/158 00:51 < 00:00, 3.04 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x15529805ba10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/projectnb/cs640grp/students/samwu/AgentO/.venv/lib64/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.6084193804606831}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6804043054580688,\n",
       " 'eval_model_preparation_time': 0.0017,\n",
       " 'eval_accuracy': {'accuracy': 0.6084193804606831},\n",
       " 'eval_runtime': 51.9427,\n",
       " 'eval_samples_per_second': 24.238,\n",
       " 'eval_steps_per_second': 3.042}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset=ds_test\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.Image.Image image mode=RGB size=111x111 at 0x155138D6C750>, <PIL.Image.Image image mode=RGB size=111x111 at 0x155138D8AA50>, <PIL.Image.Image image mode=RGB size=115x115 at 0x155138AC8B10>, <PIL.Image.Image image mode=RGB size=119x119 at 0x155139BD4A90>, <PIL.Image.Image image mode=RGB size=133x133 at 0x155138BF1590>, <PIL.Image.Image image mode=RGB size=119x119 at 0x155139BD4050>, <PIL.Image.Image image mode=RGB size=131x131 at 0x155138D898D0>, <PIL.Image.Image image mode=RGB size=129x129 at 0x155138A3F7D0>, <PIL.Image.Image image mode=RGB size=149x149 at 0x155138B3DF50>, <PIL.Image.Image image mode=RGB size=139x139 at 0x15513923C890>]\n"
     ]
    }
   ],
   "source": [
    "def process_test_set(ds, num_samples):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(num_samples):\n",
    "        byte_string = training_validation_hdf5[ds[i][\"isic_id\"]][()]\n",
    "        nparr = np.frombuffer(byte_string, np.uint8)\n",
    "        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)[...,::-1]\n",
    "        image = Image.fromarray(image)\n",
    "        inputs.append(image)\n",
    "        labels.append(ds[i][\"target\"]) \n",
    "    return inputs, labels\n",
    "\n",
    "inputs, y_true = process_test_set(ds_test, 1000)\n",
    "print(inputs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "state_dict = safetensors.torch.load_file(\"vit-finetuned-agent0/checkpoint-228/model.safetensors\")\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "image_classifier = pipeline(\"image-classification\", model,image_processor=processor)\n",
    "predictions = image_classifier(inputs)\n",
    "#select highest labels\n",
    "predictions = [max(item, key=lambda x: x['score'])['label'] for item in predictions]\n",
    "#convert from LABEL_0, LABEL_1 to 0,1\n",
    "predictions = [1 if item == 'LABEL_1' else 0 for item in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAHHCAYAAAAWM5p0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/qUlEQVR4nO3deVxV1f7/8fcB4YDIIA4gpjgr5GxdJVMzSTItTcssSzSzb4ZWombc0hwqulpZlkODiZVey0pLbZC0tJJKTY0cSBxCU3CWIEGE/fvDn+d2BJOjZ3OE83p+H/vx6Ky9ztqfzffh5cNnrbW3xTAMQwAAACbxcHUAAACgYiPZAAAApiLZAAAApiLZAAAApiLZAAAApiLZAAAApiLZAAAApiLZAAAApiLZAAAApiLZAEy0c+dOde/eXYGBgbJYLFq6dKlTx9+7d68sFouSkpKcOm55dsMNN+iGG25wdRgA/oZkAxXerl279H//939q0KCBfHx8FBAQoI4dO+qVV17RqVOnTL12bGysUlNT9eyzz+rdd9/VNddcY+r1ytLgwYNlsVgUEBBQ4s9x586dslgsslgseuGFFxwe/8CBA5o4caI2b97shGgBuFIlVwcAmGnFihW68847ZbVaNWjQIDVv3lynT5/Wd999p7Fjx2rr1q164403TLn2qVOnlJKSoieffFIjRoww5Rrh4eE6deqUvLy8TBn/YipVqqS//vpLy5YtU//+/e3OLViwQD4+PsrLy7uksQ8cOKBJkyapXr16at26dam/t3Llyku6HgDzkGygwtqzZ48GDBig8PBwrV69WrVq1bKdi4uLU3p6ulasWGHa9Q8fPixJCgoKMu0aFotFPj4+po1/MVarVR07dtR///vfYsnGwoUL1bNnT3300UdlEstff/2lypUry9vbu0yuB6D0mEZBhTV16lTl5ORo7ty5donGOY0aNdKjjz5q+3zmzBlNmTJFDRs2lNVqVb169fTvf/9b+fn5dt+rV6+eevXqpe+++07/+te/5OPjowYNGuidd96x9Zk4caLCw8MlSWPHjpXFYlG9evUknZ1+OPfffzdx4kRZLBa7tuTkZF1//fUKCgpSlSpV1LRpU/373/+2nb/Qmo3Vq1erU6dO8vPzU1BQkHr37q3t27eXeL309HQNHjxYQUFBCgwM1JAhQ/TXX39d+Ad7nnvuuUeff/65Tpw4YWtbv369du7cqXvuuadY/2PHjmnMmDFq0aKFqlSpooCAAPXo0UNbtmyx9fnmm2907bXXSpKGDBlim445d5833HCDmjdvro0bN6pz586qXLmy7edy/pqN2NhY+fj4FLv/mJgYVa1aVQcOHCj1vQK4NCQbqLCWLVumBg0a6LrrritV/wceeEATJkxQ27ZtNX36dHXp0kWJiYkaMGBAsb7p6em64447dNNNN+nFF19U1apVNXjwYG3dulWS1LdvX02fPl2SdPfdd+vdd9/Vyy+/7FD8W7duVa9evZSfn6/JkyfrxRdf1G233abvv//+H7/31VdfKSYmRocOHdLEiRMVHx+vdevWqWPHjtq7d2+x/v3799eff/6pxMRE9e/fX0lJSZo0aVKp4+zbt68sFos+/vhjW9vChQvVrFkztW3btlj/3bt3a+nSperVq5deeukljR07VqmpqerSpYvtF39ERIQmT54sSXrwwQf17rvv6t1331Xnzp1t4xw9elQ9evRQ69at9fLLL6tr164lxvfKK6+oRo0aio2NVWFhoSTp9ddf18qVK/Xqq68qLCys1PcK4BIZQAV08uRJQ5LRu3fvUvXfvHmzIcl44IEH7NrHjBljSDJWr15tawsPDzckGWvXrrW1HTp0yLBarcbo0aNtbXv27DEkGdOmTbMbMzY21ggPDy8Ww9NPP238/Z/k9OnTDUnG4cOHLxj3uWvMmzfP1ta6dWujZs2axtGjR21tW7ZsMTw8PIxBgwYVu979999vN+btt99uVKtW7YLX/Pt9+Pn5GYZhGHfccYfRrVs3wzAMo7Cw0AgNDTUmTZpU4s8gLy/PKCwsLHYfVqvVmDx5sq1t/fr1xe7tnC5duhiSjDlz5pR4rkuXLnZtX375pSHJeOaZZ4zdu3cbVapUMfr06XPRewTgHFQ2UCFlZ2dLkvz9/UvV/7PPPpMkxcfH27WPHj1akoqt7YiMjFSnTp1sn2vUqKGmTZtq9+7dlxzz+c6t9fjkk09UVFRUqu8cPHhQmzdv1uDBgxUcHGxrb9mypW666Sbbff7dQw89ZPe5U6dOOnr0qO1nWBr33HOPvvnmG2VmZmr16tXKzMwscQpFOrvOw8Pj7P/0FBYW6ujRo7Ypop9//rnU17RarRoyZEip+nbv3l3/93//p8mTJ6tv377y8fHR66+/XuprAbg8JBuokAICAiRJf/75Z6n6//777/Lw8FCjRo3s2kNDQxUUFKTff//drr1u3brFxqhataqOHz9+iREXd9ddd6ljx4564IEHFBISogEDBuiDDz74x8TjXJxNmzYtdi4iIkJHjhxRbm6uXfv591K1alVJcuhebrnlFvn7++v999/XggULdO211xb7WZ5TVFSk6dOnq3HjxrJarapevbpq1KihX375RSdPniz1NWvXru3QYtAXXnhBwcHB2rx5s2bMmKGaNWuW+rsALg/JBiqkgIAAhYWF6ddff3Xoe+cv0LwQT0/PEtsNw7jka5xbT3COr6+v1q5dq6+++kr33XeffvnlF91111266aabivW9HJdzL+dYrVb17dtX8+fP15IlSy5Y1ZCk5557TvHx8ercubPee+89ffnll0pOTtbVV19d6gqOdPbn44hNmzbp0KFDkqTU1FSHvgvg8pBsoMLq1auXdu3apZSUlIv2DQ8PV1FRkXbu3GnXnpWVpRMnTth2ljhD1apV7XZunHN+9USSPDw81K1bN7300kvatm2bnn32Wa1evVpff/11iWOfizMtLa3YuR07dqh69ery8/O7vBu4gHvuuUebNm3Sn3/+WeKi2nM+/PBDde3aVXPnztWAAQPUvXt3RUdHF/uZlDbxK43c3FwNGTJEkZGRevDBBzV16lStX7/eaeMD+GckG6iwHn/8cfn5+emBBx5QVlZWsfO7du3SK6+8IunsNICkYjtGXnrpJUlSz549nRZXw4YNdfLkSf3yyy+2toMHD2rJkiV2/Y4dO1bsu+cebnX+dtxzatWqpdatW2v+/Pl2v7x//fVXrVy50nafZujataumTJmi1157TaGhoRfs5+npWaxqsnjxYv3xxx92beeSopISM0eNGzdOGRkZmj9/vl566SXVq1dPsbGxF/w5AnAuHuqFCqthw4ZauHCh7rrrLkVERNg9QXTdunVavHixBg8eLElq1aqVYmNj9cYbb+jEiRPq0qWLfvrpJ82fP199+vS54LbKSzFgwACNGzdOt99+ux555BH99ddfmj17tpo0aWK3QHLy5Mlau3atevbsqfDwcB06dEizZs3SVVddpeuvv/6C40+bNk09evRQVFSUhg4dqlOnTunVV19VYGCgJk6c6LT7OJ+Hh4eeeuqpi/br1auXJk+erCFDhui6665TamqqFixYoAYNGtj1a9iwoYKCgjRnzhz5+/vLz89P7du3V/369R2Ka/Xq1Zo1a5aefvpp21bcefPm6YYbbtD48eM1depUh8YDcAlcvBsGMN1vv/1mDBs2zKhXr57h7e1t+Pv7Gx07djReffVVIy8vz9avoKDAmDRpklG/fn3Dy8vLqFOnjpGQkGDXxzDObn3t2bNnseucv+XyQltfDcMwVq5caTRv3tzw9vY2mjZtarz33nvFtr6uWrXK6N27txEWFmZ4e3sbYWFhxt1332389ttvxa5x/vbQr776yujYsaPh6+trBAQEGLfeequxbds2uz7nrnf+1tp58+YZkow9e/Zc8GdqGPZbXy/kQltfR48ebdSqVcvw9fU1OnbsaKSkpJS4ZfWTTz4xIiMjjUqVKtndZ5cuXYyrr766xGv+fZzs7GwjPDzcaNu2rVFQUGDXb9SoUYaHh4eRkpLyj/cA4PJZDMOBVWAAAAAOYs0GAAAwFckGAAAwFckGAAAwFckGAAAwFckGAAAwFckGAAAwFckGAAAwVYV8gqhvmxGuDgG4Ih1f/5qrQwCuOD5l8JvQWb+XTm0qn/+GqWwAAABTVcjKBgAAVxSLe/9tT7IBAIDZLBZXR+BSJBsAAJjNzSsb7n33AADAdFQ2AAAwG9MoAADAVEyjAAAAmIfKBgAAZmMaBQAAmIppFAAAAPNQ2QAAwGxMowAAAFMxjQIAAGAeKhsAAJiNaRQAAGAqN59GIdkAAMBsbl7ZcO9UCwAAmI7KBgAAZmMaBQAAmMrNkw33vnsAAGA6KhsAAJjNw70XiJJsAABgNqZRAAAAzENlAwAAs7n5czZINgAAMBvTKAAAAOahsgEAgNmYRgEAAKZy82kUkg0AAMzm5pUN9061AACA6ahsAABgNqZRAACAqZhGAQAAMA+VDQAAzMY0CgAAMBXTKAAAAOahsgEAgNmYRgEAAKZy82TDve8eAACYjsoGAABmc/MFoiQbAACYzc2nUUg2AAAwm5tXNtw71QIAAKajsgEAgNmYRgEAAKZiGgUAAMA8JBsAAJjMYrE45XDExIkTi32/WbNmtvN5eXmKi4tTtWrVVKVKFfXr109ZWVl2Y2RkZKhnz56qXLmyatasqbFjx+rMmTMO3z/TKAAAmMzRRMFZrr76an311Ve2z5Uq/e/X/qhRo7RixQotXrxYgYGBGjFihPr27avvv/9eklRYWKiePXsqNDRU69at08GDBzVo0CB5eXnpueeecygOkg0AACqoSpUqKTQ0tFj7yZMnNXfuXC1cuFA33nijJGnevHmKiIjQDz/8oA4dOmjlypXatm2bvvrqK4WEhKh169aaMmWKxo0bp4kTJ8rb27vUcTCNAgCA2SzOOfLz85WdnW135OfnX/CyO3fuVFhYmBo0aKCBAwcqIyNDkrRx40YVFBQoOjra1rdZs2aqW7euUlJSJEkpKSlq0aKFQkJCbH1iYmKUnZ2trVu3OnT7JBsAAJjMWWs2EhMTFRgYaHckJiaWeM327dsrKSlJX3zxhWbPnq09e/aoU6dO+vPPP5WZmSlvb28FBQXZfSckJESZmZmSpMzMTLtE49z5c+ccwTQKAADlREJCguLj4+3arFZriX179Ohh+++WLVuqffv2Cg8P1wcffCBfX19T4zwflQ0AAEzmrMqG1WpVQECA3XGhZON8QUFBatKkidLT0xUaGqrTp0/rxIkTdn2ysrJsazxCQ0OL7U4597mkdSD/hGQDAACTuWLr6/lycnK0a9cu1apVS+3atZOXl5dWrVplO5+WlqaMjAxFRUVJkqKiopSamqpDhw7Z+iQnJysgIECRkZEOXZtpFAAATOaKra9jxozRrbfeqvDwcB04cEBPP/20PD09dffddyswMFBDhw5VfHy8goODFRAQoJEjRyoqKkodOnSQJHXv3l2RkZG67777NHXqVGVmZuqpp55SXFxcqasp55BsAABQAe3fv1933323jh49qho1auj666/XDz/8oBo1akiSpk+fLg8PD/Xr10/5+fmKiYnRrFmzbN/39PTU8uXLNXz4cEVFRcnPz0+xsbGaPHmyw7FYDMMwnHZnVwjfNiNcHQJwRTq+/jVXhwBccXzK4M/uwHvedco4Jxfe55RxyhqVDQAATOaqJ4heKVggCgAATEVlAwAAk7l7ZYNkAwAAk7l7ssE0CgAAMBWVDQAATObulQ2SDQAAzObeuQbTKAAAwFxUNgAAMBnTKAAAwFQkGwAAwFTunmywZgMAAJiKygYAAGZz78IGyQYAAGZjGgUAAMBEVDYAADCZu1c2SDYAADCZuycbTKMAAABTUdkAAMBk7l7ZINkAAMBs7p1rMI0CAADMRWUDAACTMY0CAABMRbIBAABM5e7JBms2AACAqahsAABgNvcubJBsAABgNqZRAAAATERlAw558v9u0VMP3WLXlrYnU637PqO6tYKV9tnkEr83cOxcffzVJru24EA//fT+E6odUlWhncbqZM4p0+IGXG3um29oxssvauC9g/R4wpP644/9uqV7txL7TnvpZXWP6VHGEcJM7l7ZINmAw7amH1DPh161fT5TWCRJ2p91XPWiE+z63t+vo0YNitaX328tNs6cp+9R6s4Dqh1S1dyAARf7NfUXfbh4kZo0aWprCw2tpVXffGfX78PF72v+vLm6/vrOZR0iTEayATjoTGGRso7+Way9qMgo1n5b11b6KPln5Z46bdc+7M7rFehfWc+98bluvv5qU+MFXOmv3FwljBurpyc9ozdfn21r9/T0VPUaNez6rl71lbrf3EOV/fzKOkzAVC5NNo4cOaK3335bKSkpyszMlCSFhobquuuu0+DBg1XjvH+IuDI0qltDu1c+q7z8Av34yx5NePVT7cs8Xqxfm4g6at2sjkY9/4Fde7MGoUoY1kNdBr2gerWrl1XYgEs898xkde7cRR2irrNLNs63beuvStuxXf9+akIZRoey4u6VDZctEF2/fr2aNGmiGTNmKDAwUJ07d1bnzp0VGBioGTNmqFmzZtqwYYOrwsMFrP91rx6c8J5ui5upR557X/VqV9NXb49SlcrWYn1j+0Rp++6D+mHLHlubt1clzU8crH+/vLTEBAWoSD7/bIW2b9+mR0aNvmjfJR99qAYNGqp1m7ZlEBnKnMVJRznlssrGyJEjdeedd2rOnDnFMj7DMPTQQw9p5MiRSklJ+cdx8vPzlZ+fb//9okJZPDydHjOkld9vs/33rzsPaH3qXqV9Nln9urfV/KX/+/+Vj9VLd/W4Rs+/+YXd96c8cpvS9mRp0WfryyxmwBUyDx7U1Oef1etvvi2rtXgy/nd5eXn6/LPlGvbQw2UUHVC2XJZsbNmyRUlJSSWWliwWi0aNGqU2bdpcdJzExERNmjTJrs0z5Fp51fqX02LFhZ3MOaX0jENqWMd+yuv26Naq7OOtBct/smvvcm0TNW8UptvXt5b0v9Li/q+f13/mfqln5nxWJnEDZtu2bauOHT2qAXf2tbUVFhZq44b1WvTfBVq/KVWenmf/KEpe+YVOncrTrbf1cVG0MJu7T6O4LNkIDQ3VTz/9pGbNmpV4/qefflJISMhFx0lISFB8fLxdW81O45wSIy7Oz9db9a+qrswV9knF4D7XacWaVB05nmPXfveYt+Rr9bJ9bnd1uN6YdK+ih76s3fsOl0nMQFlo36GDPly6zK7t6ScTVK9BAw0ZOsyWaEjS0o8/0g1db1RwcHBZh4kyQrLhImPGjNGDDz6ojRs3qlu3brbEIisrS6tWrdKbb76pF1544aLjWK3WYiVKplDMkzjqdq1Ym6qMA8cUVjNQTz3UU4VFRfrgi422Pg3qVNf1bRuqz8jii+H27D9i97laUBVJ0o7dmTxnAxWKn18VNW7cxK7Nt3JlBQUG2bVn/P67Nm5Yr5mz3yjrEFGG3DzXcF2yERcXp+rVq2v69OmaNWuWCgsLJZ3dDtauXTslJSWpf//+rgoPF1A7JEjvJA5RcGBlHTmeo3Wbd6vLoBftKhixvaP0R9YJfZWyw4WRAuXD0iUfKSQkVFEdr3d1KIBpLIZhGK4OoqCgQEeOnP2Lt3r16vLy8rrIN/6Zb5sRzggLqHCOr3/N1SEAVxyfMvizu/HYLy7eqRR2TrvZKeOUtSvioV5eXl6qVauWq8MAAMAU7j6NwovYAACAqa6IygYAABUZu1EAAICp3DzXYBoFAACYi8oGAAAm8/Bw79IGyQYAACZjGgUAAMBEVDYAADAZu1EAAICp3DzXINkAAMBs7l7ZYM0GAAAwFZUNAABM5u6VDZINAABM5ua5BtMoAADAXFQ2AAAwGdMoAADAVG6eazCNAgAAzEVlAwAAkzGNAgAATOXmuQbTKAAAuIPnn39eFotFjz32mK0tLy9PcXFxqlatmqpUqaJ+/fopKyvL7nsZGRnq2bOnKleurJo1a2rs2LE6c+aMQ9cm2QAAwGQWi8Upx6Vav369Xn/9dbVs2dKufdSoUVq2bJkWL16sNWvW6MCBA+rbt6/tfGFhoXr27KnTp09r3bp1mj9/vpKSkjRhwgSHrk+yAQCAySwW5xyXIicnRwMHDtSbb76pqlWr2tpPnjypuXPn6qWXXtKNN96odu3aad68eVq3bp1++OEHSdLKlSu1bds2vffee2rdurV69OihKVOmaObMmTp9+nSpYyDZAADAZM6qbOTn5ys7O9vuyM/P/8drx8XFqWfPnoqOjrZr37hxowoKCuzamzVrprp16yolJUWSlJKSohYtWigkJMTWJyYmRtnZ2dq6dWup759kAwCAciIxMVGBgYF2R2Ji4gX7L1q0SD///HOJfTIzM+Xt7a2goCC79pCQEGVmZtr6/D3ROHf+3LnSYjcKAAAmc9ZulISEBMXHx9u1Wa3WEvvu27dPjz76qJKTk+Xj4+OcAC4RlQ0AAEzmrGkUq9WqgIAAu+NCycbGjRt16NAhtW3bVpUqVVKlSpW0Zs0azZgxQ5UqVVJISIhOnz6tEydO2H0vKytLoaGhkqTQ0NBiu1POfT7XpzRINgAAqIC6deum1NRUbd682XZcc801GjhwoO2/vby8tGrVKtt30tLSlJGRoaioKElSVFSUUlNTdejQIVuf5ORkBQQEKDIystSxMI0CAIDJXPFQL39/fzVv3tyuzc/PT9WqVbO1Dx06VPHx8QoODlZAQIBGjhypqKgodejQQZLUvXt3RUZG6r777tPUqVOVmZmpp556SnFxcResqJSEZAMAAJNdqY8rnz59ujw8PNSvXz/l5+crJiZGs2bNsp339PTU8uXLNXz4cEVFRcnPz0+xsbGaPHmyQ9exGIZhODt4V/NtM8LVIQBXpOPrX3N1CMAVx6cM/uzuOO1bp4zz/dhOThmnrFHZAADAZFdoYaPMkGwAAGCyK3UapaywGwUAAJiKygYAACZz98oGyQYAACZz81yDZAMAALO5e2WDNRsAAMBUVDYAADCZmxc2SDYAADAb0ygAAAAmorIBAIDJ3LywQbIBAIDZPNw822AaBQAAmIrKBgAAJnPzwgbJBgAAZnP33SgkGwAAmMzDvXMN1mwAAABzUdkAAMBkTKMAAABTuXmuwTQKAAAwl1OSjRMnTjhjGAAAKiSLk/6vvHI42fjPf/6j999/3/a5f//+qlatmmrXrq0tW7Y4NTgAACoCD4tzjvLK4WRjzpw5qlOnjiQpOTlZycnJ+vzzz9WjRw+NHTvW6QECAIDyzeEFopmZmbZkY/ny5erfv7+6d++uevXqqX379k4PEACA8s7dd6M4XNmoWrWq9u3bJ0n64osvFB0dLUkyDEOFhYXOjQ4AgArAYnHOUV45XNno27ev7rnnHjVu3FhHjx5Vjx49JEmbNm1So0aNnB4gAAAo3xxONqZPn6569epp3759mjp1qqpUqSJJOnjwoB5++GGnBwgAQHnn7q+YdzjZ8PLy0pgxY4q1jxo1yikBAQBQ0bh5rlG6ZOPTTz8t9YC33XbbJQcDAEBF5O4LREuVbPTp06dUg1ksFhaJAgAAO6VKNoqKisyOAwCACsvNCxuX9yK2vLw8+fj4OCsWAAAqJHdfIOrwczYKCws1ZcoU1a5dW1WqVNHu3bslSePHj9fcuXOdHiAAACjfHE42nn32WSUlJWnq1Kny9va2tTdv3lxvvfWWU4MDAKAisDjpKK8cTjbeeecdvfHGGxo4cKA8PT1t7a1atdKOHTucGhwAABWBxWJxylFeOZxs/PHHHyU+KbSoqEgFBQVOCQoAAFQcDicbkZGR+vbbb4u1f/jhh2rTpo1TggIAoCJx91fMO7wbZcKECYqNjdUff/yhoqIiffzxx0pLS9M777yj5cuXmxEjAADlWnmeAnEGhysbvXv31rJly/TVV1/Jz89PEyZM0Pbt27Vs2TLddNNNZsQIAADKsUt6zkanTp2UnJzs7FgAAKiQ3LywcekP9dqwYYO2b98u6ew6jnbt2jktKAAAKhJ3n0ZxONnYv3+/7r77bn3//fcKCgqSJJ04cULXXXedFi1apKuuusrZMQIAUK6V58WdzuDwmo0HHnhABQUF2r59u44dO6Zjx45p+/btKioq0gMPPGBGjAAAoBxzuLKxZs0arVu3Tk2bNrW1NW3aVK+++qo6derk1OAAAKgImEZxUJ06dUp8eFdhYaHCwsKcEhQAABWJe6calzCNMm3aNI0cOVIbNmywtW3YsEGPPvqoXnjhBacGBwAAyr9SVTaqVq1qVwLKzc1V+/btVanS2a+fOXNGlSpV0v33368+ffqYEigAAOWVu79ivlTJxssvv2xyGAAAVFxunmuULtmIjY01Ow4AAFBBXfJDvSQpLy9Pp0+ftmsLCAi4rIAAAKho3H03isMLRHNzczVixAjVrFlTfn5+qlq1qt0BAADsWSzOOcorh5ONxx9/XKtXr9bs2bNltVr11ltvadKkSQoLC9M777xjRowAAKAcc3gaZdmyZXrnnXd0ww03aMiQIerUqZMaNWqk8PBwLViwQAMHDjQjTgAAyi13343icGXj2LFjatCggaSz6zOOHTsmSbr++uu1du1a50YHAEAFwDSKgxo0aKA9e/ZIkpo1a6YPPvhA0tmKx7kXswEAgP+xWCxOOcorh5ONIUOGaMuWLZKkJ554QjNnzpSPj49GjRqlsWPHOj1AAABQvlkMwzAuZ4Dff/9dGzduVKNGjdSyZUtnxXVZMo7luzoE4Iq0aMt+V4cAXHEe79rQ9GuMXLLdKeO8enuEU8Ypa5f1nA1JCg8PV3h4uDNiAQCgQirPUyDOUKpkY8aMGaUe8JFHHrnkYAAAQMVTqmRj+vTppRrMYrGQbAAAcB4PFxQ2Zs+erdmzZ2vv3r2SpKuvvloTJkxQjx49JJ19Cvjo0aO1aNEi5efnKyYmRrNmzVJISIhtjIyMDA0fPlxff/21qlSpotjYWCUmJtpexFpapep9bvcJAABwnCuSjauuukrPP/+8GjduLMMwNH/+fPXu3VubNm3S1VdfrVGjRmnFihVavHixAgMDNWLECPXt21fff/+9JKmwsFA9e/ZUaGio1q1bp4MHD2rQoEHy8vLSc88951Asl71A9ErEAlGgZCwQBYoriwWi8Z/ucMo4L93W7LK+HxwcrGnTpumOO+5QjRo1tHDhQt1xxx2SpB07digiIkIpKSnq0KGDPv/8c/Xq1UsHDhywVTvmzJmjcePG6fDhw/L29i71dR3e+goAABzjrOds5OfnKzs72+7Iz7/4H9iFhYVatGiRcnNzFRUVpY0bN6qgoEDR0dG2Ps2aNVPdunWVkpIiSUpJSVGLFi3splViYmKUnZ2trVu3OnT/JBsAAJjMw+KcIzExUYGBgXZHYmLiBa+bmpqqKlWqyGq16qGHHtKSJUsUGRmpzMxMeXt7F3sYZ0hIiDIzMyVJmZmZdonGufPnzjnisre+AgCAspGQkKD4+Hi7NqvVesH+TZs21ebNm3Xy5El9+OGHio2N1Zo1a8wOsxiSDQAATOasx2xYrdZ/TC7O5+3trUaNGkmS2rVrp/Xr1+uVV17RXXfdpdOnT+vEiRN21Y2srCyFhoZKkkJDQ/XTTz/ZjZeVlWU754hLmkb59ttvde+99yoqKkp//PGHJOndd9/Vd999dynDAQBQoXlYLE45LldRUZHy8/PVrl07eXl5adWqVbZzaWlpysjIUFRUlCQpKipKqampOnTokK1PcnKyAgICFBkZ6dj9OxroRx99pJiYGPn6+mrTpk22hSknT550eCsMAADuwMNJhyMSEhK0du1a7d27V6mpqUpISNA333yjgQMHKjAwUEOHDlV8fLy+/vprbdy4UUOGDFFUVJQ6dOggSerevbsiIyN13333acuWLfryyy/11FNPKS4uzqHqyrn7d8gzzzyjOXPm6M0335SXl5etvWPHjvr5558dHQ4AAJjg0KFDGjRokJo2bapu3bpp/fr1+vLLL3XTTTdJOvvAzl69eqlfv37q3LmzQkND9fHHH9u+7+npqeXLl8vT01NRUVG69957NWjQIE2ePNnhWBxes5GWlqbOnTsXaw8MDNSJEyccDgAAgIrOFa9GmTt37j+e9/Hx0cyZMzVz5swL9gkPD9dnn3122bE4XNkIDQ1Venp6sfbvvvtODRo0uOyAAACoaK6UNRuu4nCyMWzYMD366KP68ccfZbFYdODAAS1YsEBjxozR8OHDzYgRAACUYw5PozzxxBMqKipSt27d9Ndff6lz586yWq0aM2aMRo4caUaMAACUa+W4KOEUDicbFotFTz75pMaOHav09HTl5OQoMjJSVapUMSM+AADKPVe8iO1KcskP9fL29nZ4ny0AAHA/DicbXbt2leUf6kGrV6++rIAAAKhoyvPiTmdwONlo3bq13eeCggJt3rxZv/76q2JjY50VFwAAFYab5xqOJxvTp08vsX3ixInKycm57IAAAEDF4rRXzN977716++23nTUcAAAVhrNeMV9eOe2trykpKfLx8XHWcAAAVBgWleNMwQkcTjb69u1r99kwDB08eFAbNmzQ+PHjnRYYAAAVRXmuSjiDw8lGYGCg3WcPDw81bdpUkydPVvfu3Z0WGAAAqBgcSjYKCws1ZMgQtWjRQlWrVjUrJgAAKhR3r2w4tEDU09NT3bt35+2uAAA4wGKxOOUorxzejdK8eXPt3r3bjFgAAEAF5HCy8cwzz2jMmDFavny5Dh48qOzsbLsDAADYY+trKU2ePFmjR4/WLbfcIkm67bbb7Eo6hmHIYrGosLDQ+VECAFCOleMZEKcodbIxadIkPfTQQ/r666/NjAcAAFQwpU42DMOQJHXp0sW0YAAAqIh4EZsDyvNKWAAAXKU8r7dwBoeSjSZNmlw04Th27NhlBQQAACoWh5KNSZMmFXuCKAAA+GfuPjHgULIxYMAA1axZ06xYAACokDx4EVvpsF4DAIBL4+6/Qkv9UK9zu1EAAAAcUerKRlFRkZlxAABQYbEbBQAAmMrdn7Ph8LtRAAAAHEFlAwAAk7l5YYNkAwAAszGNAgAAYCIqGwAAmMzNCxskGwAAmM3dpxHc/f4BAIDJqGwAAGAyd3/lB8kGAAAmc+9Ug2QDAADTsfUVAADARFQ2AAAwmXvXNUg2AAAwnZvPojCNAgAAzEVlAwAAk7H1FQAAmMrdpxHc/f4BAIDJqGwAAGAyplEAAICp3DvVYBoFAACYjMoGAAAmYxoFAACYyt2nEUg2AAAwmbtXNtw92QIAACajsgEAgMncu65BsgEAgOncfBaFaRQAAGAuKhsAAJjMw80nUkg2AAAwGdMoAAAAJqKyAQCAySxMowAAADMxjQIAACqcxMREXXvttfL391fNmjXVp08fpaWl2fXJy8tTXFycqlWrpipVqqhfv37Kysqy65ORkaGePXuqcuXKqlmzpsaOHaszZ844FAvJBgAAJvOQxSmHI9asWaO4uDj98MMPSk5OVkFBgbp3767c3Fxbn1GjRmnZsmVavHix1qxZowMHDqhv376284WFherZs6dOnz6tdevWaf78+UpKStKECRMcisViGIbh0DfKgYxj+a4OAbgiLdqy39UhAFecx7s2NP0aX2477JRxYiJrXPJ3Dx8+rJo1a2rNmjXq3LmzTp48qRo1amjhwoW64447JEk7duxQRESEUlJS1KFDB33++efq1auXDhw4oJCQEEnSnDlzNG7cOB0+fFje3t6lujaVDQAATGaxOOfIz89Xdna23ZGfX7o/sE+ePClJCg4OliRt3LhRBQUFio6OtvVp1qyZ6tatq5SUFElSSkqKWrRoYUs0JCkmJkbZ2dnaunVrqe+fZAMAgHIiMTFRgYGBdkdiYuJFv1dUVKTHHntMHTt2VPPmzSVJmZmZ8vb2VlBQkF3fkJAQZWZm2vr8PdE4d/7cudJiNwoAACZz1tbXhIQExcfH27VZrdaLfi8uLk6//vqrvvvuO6fE4SiSDQAATObhpK2vVqu1VMnF340YMULLly/X2rVrddVVV9naQ0NDdfr0aZ04ccKuupGVlaXQ0FBbn59++sluvHO7Vc71KQ2mUQAAqIAMw9CIESO0ZMkSrV69WvXr17c7365dO3l5eWnVqlW2trS0NGVkZCgqKkqSFBUVpdTUVB06dMjWJzk5WQEBAYqMjCx1LFQ2AAAwmSueIBoXF6eFCxfqk08+kb+/v22NRWBgoHx9fRUYGKihQ4cqPj5ewcHBCggI0MiRIxUVFaUOHTpIkrp3767IyEjdd999mjp1qjIzM/XUU08pLi7OoQoLyQYAACZzxRNEZ8+eLUm64YYb7NrnzZunwYMHS5KmT58uDw8P9evXT/n5+YqJidGsWbNsfT09PbV8+XINHz5cUVFR8vPzU2xsrCZPnuxQLDxnA3AjPGcDKK4snrPxddpRp4zTtWk1p4xT1qhsAABgMl7EBgAATOWs3SjlFbtRAACAqahswGG/bNqgxQuS9Fvadh07clgTn39ZHbvcaDtvGIbmvzlLn3/6kXL+/FNXt2ytRx5/SlfVCbcb58fv1+q9t+dod/pOeVu91bLNNZr0n1fK+nYAp9jyxfvau2mdTmbul6e3t2o2iNC1t9+voNCzzzX480iWPnhqSInfvXFYguq366S8nGx98/Y0Hf9jj/Jys+XrH6S6LTvomj6D5e1buSxvB07GNArgoLy8U2rQuKliet2uSQmjip1//715Wrp4oR4f/4xCw2or6Y3XlPDYQ5q7cKm8//9WqW+/Ttb0xEka8tAjanPNv1RYWKi9u9LL+lYApzn426+K6NJLNeo1UVFRoTYsna8vZjypfk+/Li+rj/yCq+vu/7xn9520775Q6sqPdNXV10iSLBaLwlt1ULve98mnSqD+PHxQ6/47S98vfFVdh45zxW3BSVyxG+VKQrIBh/0rqpP+FdWpxHOGYWjJ++9p4OBhuq5zV0nSuAnP6s6eXfX92tXqelMPFZ45o1nT/6NhI+LV47b/vco4vL75K8IBs9z8yBS7z51j47Vw7N06krFTtRq3kIeHpyoHBtv12bt5neq36yQvH19JktXPXxFdetrO+1cLUUSXnkpN/sj8G4Cp3DzXYM0GnCvzwB86dvSI2lzbwdbmV8VfzSJbaNuvWyRJO9O268jhQ7J4eOihQf11V68b9e9Rw7Vn105XhQ04XcGpXEmStbJ/ieeP/L5Tx/btVpOO3S84Ru6Jo9q7aZ1CG7cwJUagrFzRyca+fft0//33/2Ofy3ndLpzv2NEjkqSqwfZ7wasGV9Pxo2f3mR88cPZZD+/Ona2BQ4ZpyguvqYp/gMbEDVX2/38FMlCeGUVF+mHx6wppGKng2vVK7JP2/UoFhdZRSMPij3z++q3/KGnk7Vr0xH3y9q2s6+971OSIYTYPi8UpR3l1RScbx44d0/z58/+xT0mv25318tQyihCXwigqkiTdEztMnbrepCbNIjXmqSmyWCxau3qli6MDLt+6RbN0/I/f1fWBJ0o8f+Z0vnav/0ZNOsaUeL79ncPU58kZih4+QdmHD+rHxW+aGS7KgMVJR3nl0jUbn3766T+e371790XHKOl1u1m5lxUWLkNwteqSpOPHjqpa9Rq29uPHjqphk6Zn+/z/9vD6DWznvb29VSustg5lHSzDaAHnW/ffWdqX+pN6jp4qv6rVS+yz5+fvdOZ0vhp16Fbi+cqBwaocGKyg0Dqy+vlrxQtj1abn3cXWfADlhUuTjT59+shiseifnphuuUjZqKTX7Z44wzSKq4SG1VZwteratOFHNWrSTJKUm5ujHdtSdWvf/pKkxs0i5eXtrX2/71XzVm0lSWfOFCjz4AGFhIa5LHbgchiGoZRFs/X75hTdEv+8/Ktf+PXbv32/UnVbtpevf2Apxj1bCSwsKHBarHCB8lyWcAKXJhu1atXSrFmz1Lt37xLPb968We3atSvjqHAxp/76S3/sz7B9zjzwh9J/26GAgEDVDK2l2++6VwuT3lDtOnVVq1ZtJb05U9Wq11DHzmefxeHnV0W9+typd96apRohoQoJraUPFiRJkjrfeOHFcsCVbN1/Z2n3+m8UPXyCvHx89dfJY5Ikb18/VfL+3x9E2YcOKDP9V8WMmFRsjH2p63Xqz+OqHt5EXlZfHT/4u9Z/NFchDSPlXz2kzO4FzsdzNlyoXbt22rhx4wWTjYtVPeAav+3YqjFxQ22f58yYJkm66Zbb9Pj4Z3TXvUOUd+qUXn5+snJy/lTzlm2UOH227RkbkvTgyHh5enrqP5P+rdP5+Wp2dQtNe+0t+QcElPn9AM6wY+0KSdJnL9k/D6PToFFqct1Nts+/rVspv6Dqqh3RttgYnt7eSvvuS/24+E0VnimQX9Xqqtemo1rG3Glu8IDJXPrW12+//Va5ubm6+eabSzyfm5urDRs2qEuXLg6Ny1tfgZLx1leguLJ46+tPu52z0+5fDS4+9XYlcmllo1Onkh8MdY6fn5/DiQYAAFca955EucK3vgIAgPKPx5UDAGA2Ny9tkGwAAGAydqMAAABTleMnjTsFazYAAICpqGwAAGAyNy9skGwAAGA6N882mEYBAACmorIBAIDJ2I0CAABMxW4UAAAAE1HZAADAZG5e2CDZAADAdG6ebTCNAgAATEVlAwAAk7EbBQAAmMrdd6OQbAAAYDI3zzVYswEAAMxFZQMAALO5eWmDZAMAAJO5+wJRplEAAICpqGwAAGAydqMAAABTuXmuwTQKAAAwF5UNAADM5ualDZINAABMxm4UAAAAE1HZAADAZOxGAQAApnLzXINkAwAA07l5tsGaDQAAYCoqGwAAmMzdd6OQbAAAYDJ3XyDKNAoAADAVlQ0AAEzm5oUNkg0AAEzn5tkG0ygAAMBUVDYAADAZu1EAAICp2I0CAABgIiobAACYzM0LGyQbAACYzs2zDZINAABM5u4LRFmzAQAATEWyAQCAySwW5xyOWrt2rW699VaFhYXJYrFo6dKlducNw9CECRNUq1Yt+fr6Kjo6Wjt37rTrc+zYMQ0cOFABAQEKCgrS0KFDlZOT41AcJBsAAJjM4qTDUbm5uWrVqpVmzpxZ4vmpU6dqxowZmjNnjn788Uf5+fkpJiZGeXl5tj4DBw7U1q1blZycrOXLl2vt2rV68MEHHYrDYhiGcQnxX9EyjuW7OgTgirRoy35XhwBccR7v2tD0a+xz0u+lOsHWS/6uxWLRkiVL1KdPH0lnqxphYWEaPXq0xowZI0k6efKkQkJClJSUpAEDBmj79u2KjIzU+vXrdc0110iSvvjiC91yyy3av3+/wsLCSnVtKhsAAJjMVdMo/2TPnj3KzMxUdHS0rS0wMFDt27dXSkqKJCklJUVBQUG2REOSoqOj5eHhoR9//LHU12I3CgAApnNOppCfn6/8fPsqidVqldXqeMUjMzNTkhQSEmLXHhISYjuXmZmpmjVr2p2vVKmSgoODbX1Kg8oGAADlRGJiogIDA+2OxMREV4d1UVQ2AAAwmbOmQBISEhQfH2/XdilVDUkKDQ2VJGVlZalWrVq29qysLLVu3drW59ChQ3bfO3PmjI4dO2b7fmlQ2QAAwGTO2o1itVoVEBBgd1xqslG/fn2FhoZq1apVtrbs7Gz9+OOPioqKkiRFRUXpxIkT2rhxo63P6tWrVVRUpPbt25f6WlQ2AACooHJycpSenm77vGfPHm3evFnBwcGqW7euHnvsMT3zzDNq3Lix6tevr/HjxyssLMy2YyUiIkI333yzhg0bpjlz5qigoEAjRozQgAEDSr0TRSLZAADAdK56xfyGDRvUtWtX2+dzUzCxsbFKSkrS448/rtzcXD344IM6ceKErr/+en3xxRfy8fGxfWfBggUaMWKEunXrJg8PD/Xr108zZsxwKA6eswG4EZ6zARRXFs/ZyDxZ4JRxQgO9nDJOWaOyAQCA2dz7PWwsEAUAAOaisgEAgMncvLBBsgEAgNlctUD0SsE0CgAAMBWVDQAATGZx84kUkg0AAMzm3rkG0ygAAMBcVDYAADCZmxc2SDYAADAbu1EAAABMRGUDAACTsRsFAACYimkUAAAAE5FsAAAAUzGNAgCAydx9GoVkAwAAk7n7AlGmUQAAgKmobAAAYDKmUQAAgKncPNdgGgUAAJiLygYAAGZz89IGyQYAACZjNwoAAICJqGwAAGAydqMAAABTuXmuQbIBAIDp3DzbYM0GAAAwFZUNAABM5u67UUg2AAAwmbsvEGUaBQAAmMpiGIbh6iBQMeXn5ysxMVEJCQmyWq2uDge4YvBvA+6GZAOmyc7OVmBgoE6ePKmAgABXhwNcMfi3AXfDNAoAADAVyQYAADAVyQYAADAVyQZMY7Va9fTTT7MADjgP/zbgblggCgAATEVlAwAAmIpkAwAAmIpkAwAAmIpkAwAAmIpkA6aZOXOm6tWrJx8fH7Vv314//fSTq0MCXGrt2rW69dZbFRYWJovFoqVLl7o6JKBMkGzAFO+//77i4+P19NNP6+eff1arVq0UExOjQ4cOuTo0wGVyc3PVqlUrzZw509WhAGWKra8wRfv27XXttdfqtddekyQVFRWpTp06GjlypJ544gkXRwe4nsVi0ZIlS9SnTx9XhwKYjsoGnO706dPauHGjoqOjbW0eHh6Kjo5WSkqKCyMDALgCyQac7siRIyosLFRISIhde0hIiDIzM10UFQDAVUg2AACAqUg24HTVq1eXp6ensrKy7NqzsrIUGhrqoqgAAK5CsgGn8/b2Vrt27bRq1SpbW1FRkVatWqWoqCgXRgYAcIVKrg4AFVN8fLxiY2N1zTXX6F//+pdefvll5ebmasiQIa4ODXCZnJwcpaen2z7v2bNHmzdvVnBwsOrWrevCyABzsfUVpnnttdc0bdo0ZWZmqnXr1poxY4bat2/v6rAAl/nmm2/UtWvXYu2xsbFKSkoq+4CAMkKyAQAATMWaDQAAYCqSDQAAYCqSDQAAYCqSDQAAYCqSDQAAYCqSDQAAYCqSDQAAYCqSDcCFBg8erD59+tg+33DDDXrsscfKPI5vvvlGFotFJ06cuGAfi8WipUuXlnrMiRMnqnXr1pcV1969e2WxWLR58+bLGgeAa5FsAOcZPHiwLBaLLBaLvL291ahRI02ePFlnzpwx/doff/yxpkyZUqq+pUkQAOBKwLtRgBLcfPPNmjdvnvLz8/XZZ58pLi5OXl5eSkhIKNb39OnT8vb2dsp1g4ODnTIOAFxJqGwAJbBarQoNDVV4eLiGDx+u6Ohoffrpp5L+N/Xx7LPPKiwsTE2bNpUk7du3T/3791dQUJCCg4PVu3dv7d271zZmYWGh4uPjFRQUpGrVqunxxx/X+W8LOH8aJT8/X+PGjVOdOnVktVrVqFEjzZ07V3v37rW9Y6Nq1aqyWCwaPHiwpLNv2E1MTFT9+vXl6+urVq1a6cMPP7S7zmeffaYmTZrI19dXXbt2tYuztMaNG6cmTZqocuXKatCggcaPH6+CgoJi/V5//XXVqVNHlStXVv/+/XXy5Em782+99ZYiIiLk4+OjZs2aadasWRe85vHjxzVw4EDVqFFDvr6+aty4sebNm+dw7ADKFpUNoBR8fX119OhR2+dVq1YpICBAycnJkqSCggLFxMQoKipK3377rSpVqqRnnnlGN998s3755Rd5e3vrxRdfVFJSkt5++21FREToxRdf1JIlS3TjjTde8LqDBg1SSkqKZsyYoVatWmnPnj06cuSI6tSpo48++kj9+vVTWlqaAgIC5OvrK0lKTEzUe++9pzlz5qhx48Zau3at7r33XtWoUUNdunTRvn371LdvX8XFxenBBx/Uhg0bNHr0aId/Jv7+/kpKSlJYWJhSU1M1bNgw+fv76/HHH7f1SU9P1wcffKBly5YpOztbQ4cO1cMPP6wFCxZIkhYsWKAJEybotddeU5s2bbRp0yYNGzZMfn5+io2NLXbN8ePHa9u2bfr8889VvXp1paen69SpUw7HDqCMGQDsxMbGGr179zYMwzCKioqM5ORkw2q1GmPGjLGdDwkJMfLz823feffdd42mTZsaRUVFtrb8/HzD19fX+PLLLw3DMIxatWoZU6dOtZ0vKCgwrrrqKtu1DMMwunTpYjz66KOGYRhGWlqaIclITk4uMc6vv/7akGQcP37c1paXl2dUrlzZWLdunV3foUOHGnfffbdhGIaRkJBgREZG2p0fN25csbHOJ8lYsmTJBc9PmzbNaNeune3z008/bXh6ehr79++3tX3++eeGh4eHcfDgQcMwDKNhw4bGwoUL7caZMmWKERUVZRiGYezZs8eQZGzatMkwDMO49dZbjSFDhlwwBgBXJiobQAmWL1+uKlWqqKCgQEVFRbrnnns0ceJE2/kWLVrYrdPYsmWL0tPT5e/vbzdOXl6edu3apZMnT+rgwYNq37697VylSpV0zTXXFJtKOWfz5s3y9PRUly5dSh13enq6/vrrL91000127adPn1abNm0kSdu3b7eLQ5KioqJKfY1z3n//fc2YMUO7du1STk6Ozpw5o4CAALs+devWVe3ate2uU1RUpLS0NPn7+2vXrl0aOnSohg0bZutz5swZBQYGlnjN4cOHq1+/fvr555/VvXt39enTR9ddd53DsQMoWyQbQAm6du2q2bNny9vbW2FhYapUyf6fip+fn93nnJwctWvXzjY98Hc1atS4pBjOTYs4IicnR5K0YsUKu1/y0tl1KM6SkpKigQMHatKkSYqJiVFgYKAWLVqkF1980eFY33zzzWLJj6enZ4nf6dGjh37//Xd99tlnSk5OVrdu3RQXF6cXXnjh0m8GgOlINoAS+Pn5qVGjRqXu37ZtW73//vuqWbNmsb/uz6lVq5Z+/PFHde7cWdLZv+A3btyotm3blti/RYsWKioq0po1axQdHV3s/LnKSmFhoa0tMjJSVqtVGRkZF6yIRERE2Ba7nvPDDz9c/Cb/Zt26dQoPD9eTTz5pa/v999+L9cvIyNCBAwcUFhZmu46Hh4eaNm2qkJAQhYWFaffu3Ro4cGCpr12jRg3FxsYqNjZWnTp10tixY0k2gCscu1EAJxg4cKCqV6+u3r1769tvv9WePXv0zTff6JFHHtH+/fslSY8++qief/55LV26VDt27NDDDz/8j8/IqFevnmJjY3X//fdr6dKltjE/+OADSVJ4eLgsFouWL1+uw4cPKycnR/7+/hozZoxGjRql+fPna9euXfr555/16quvav78+ZKkhx56SDt37tTYsWOVlpamhQsXKikpyaH7bdy4sTIyMrRo0SLt2rVLM2bM0JIlS4r18/HxUWxsrLZs2aJvv/1WjzzyiPr376/Q0FBJ0qRJk5SYmKgZM2bot99+U2pqqubNm6eXXnqpxOtOmDBBn3zyidLT07V161YtX75cERERDsUOoOyRbABOULlyZa1du1Z169ZV3759FRERoaFDhyovL89W6Rg9erTuu+8+xcbGKioqSv7+/rr99tv/cdzZs2frjjvu0MMPP6xmzZpp2LBhys3NlSTVrl1bkyZN0hNPPKGQkBCNGDFCkjRlyhSNHz9eiYmJioiI0M0336wVK1aofv36ks6uo/joo4+0dOlStWrVSnPmzNFzzz3n0P3edtttGjVqlEaMGKHWrVtr3bp1Gj9+fLF+jRo1Ut++fXXLLbeoe/fuatmypd3W1gceeEBvvfWW5s2bpxYtWqhLly5KSkqyxXo+b29vJSQkqGXLlurcubM8PT21aNEih2IHUPYsxoVWpwEAADgBlQ0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGAqkg0AAGCq/wc+8y+ArrS6WgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "confusion_metric = evaluate.load(\"confusion_matrix\")\n",
    "confusion_matrix = confusion_metric.compute(predictions=predictions, references=y_true)\n",
    "matrix = confusion_matrix['confusion_matrix']\n",
    "if 'labels' in confusion_matrix:\n",
    "    labels = confusion_matrix['labels']\n",
    "else:\n",
    "    labels = np.unique(predictions + y_true)\n",
    "sns.heatmap(matrix, annot=True, fmt=\"d\", xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
